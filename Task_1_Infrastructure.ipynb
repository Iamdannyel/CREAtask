{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Infrastructure Monitoring (internal): In the Russia project, we run processes on a daily basis and need to know whether they have run successfully or not. Please suggest one or several configurations that would allow us (internally) to check the success of each process. We are looking for setups that are both easy to maintain and generate as little friction / efforts as possible for us to check.**\n",
        "\n",
        "I suggest the use of ELK Stack for log/ process monitoring. The ELK Stack comprises three tools - Elasticsearch, Logstash, and Kibana which when combined provide an end-to-end centralized log management system that allows us to monitor the success of each process in the projectâ€™s pipeline. Elasticsearch is the core component of the stack, it indexes and ensures quick access to log data for analysis and monitoring. The Logstash is the log collection and processing component of the stack. It collects logs from various sources, parses them, enriches them, and then sends them to Elasticsearch for indexing. The final component, Kibana allows visual exploration of indexed log data through the creation of dashboards. \n",
        "\n",
        "\n",
        "Configuring and setting up the ELK Stack\n",
        "\n",
        "Install Elasticsearch: Download and install Elasticsearch on the local machine following the official Elasticsearch documentation.\n",
        "\n",
        "Configure Elasticsearch: Modify the Elasticsearch configuration file to specify settings such as cluster name, node roles, network settings, and storage paths. Customize these settings based on the project requirements and available resources.\n",
        "\n",
        "Install Logstash: Download and install Logstash on the same server or machine where Elasticsearch is installed.\n",
        "\n",
        "Configure Logstash: Create a Logstash configuration file where you define input sources, filters for log processing, and output settings to direct the processed logs to Elasticsearch. Customize the configuration based on your log sources, formats, and desired transformations.\n",
        "\n",
        "Install Kibana: Download and install Kibana on the same server or machine where Elasticsearch is installed.\n",
        "\n",
        "Configure Kibana: Modify the Kibana configuration file to specify Elasticsearch cluster details, such as the host and port. Optionally, configure authentication, SSL/TLS settings, and other advanced configurations based on the security requirements.\n",
        "\n",
        "Start Services: Start Elasticsearch, Logstash, and Kibana services using the appropriate commands or service managers specific to the operating system.\n",
        "\n",
        "Identify Key Metrics: Determine the important metrics and indicators that define the success of each process in your pipeline. These could include metrics such as data ingestion rates, data quality checks, analysis completion times, error rates, or any other relevant performance indicators.\n",
        "\n",
        "Instrument Logging: Instrument your pipeline processes with appropriate logging statements to capture relevant information and events at different stages. This could involve logging the start and end times of each process, capturing error messages, logging data quality checks, or any other relevant information that helps assess the success of a particular process.\n",
        "\n",
        "Monitor and Analyze: Start sending log data to Logstash, which will process and send it to Elasticsearch for indexing. Use Kibana to monitor the indexed logs, analyze metrics, search for specific events or patterns, and track the success of each process in your project.\n",
        "\n",
        "\n",
        "Justification for choice of ELK Stack for monitoring process \n",
        "\n",
        "\n",
        "1. Scalability and Performance: The ELK stack efficiently handles the large volumes of trade data generated by your pipeline, ensuring smooth processing, analysis, and tracking of fossil fuel trades.\n",
        "\n",
        "2. Flexibility and Extensibility: With the ELK stack, you can easily adapt to different trade data sources and formats, allowing seamless integration of various trading platforms, data feeds, and internal systems.\n",
        "\n",
        "3. Powerful Search and Analysis: The ELK stack empowers you to perform in-depth analysis on pricing, volumes, market trends, and other trade-related metrics specific to fossil fuel trades, enabling comprehensive tracking and analysis of the collected data.\n",
        "\n",
        "4. Real-time Monitoring and Alerting: By leveraging the ELK stack's real-time monitoring and alerting capabilities, you can promptly detect anomalies, monitor the success of each trade process, and receive alerts for critical events or potential errors, ensuring the integrity and reliability of the trade data pipeline.\n",
        "\n",
        "5. Ecosystem and Community Support: The ELK stack's vibrant open-source community provides extensive resources, plugins, and documentation tailored to various industries, including energy and finance, making it well-suited for the specific requirements of tracking fossil fuel trades.\n",
        "\n",
        "6. Integration with Other Tools: The ELK stack seamlessly integrates with other data engineering and data science tools, allowing you to combine trade data with additional datasets, leverage advanced analytics, and integrate with existing workflows specific to fossil fuel trade analysis.\n",
        "\n",
        "Considering the project's objective and requirement of maintaining databases and pipelines for tracking and analysing trades of fossil fuels, the ELK stack offers the necessary capabilities to efficiently manage logs, monitor processes, and detect errors quickly to optimize pipeline performance. "
      ],
      "metadata": {
        "id": "ikE0UDF8tURz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. **Alerts (external): External users of our platform should be able to set alerts for when potential fraudulent shipments are detected. Assuming the shipments are created in bulk and stored in a Postgres database query, design an infrastructure that would allow user to create / edit alert criteria and receive them when our system detects them. Please describe the setup in detail.**\n",
        "Step 1 - Create a Database Schema: Design a schema for your Postgres database that matches the structure of the shipment data. Define tables and columns that correspond to the relevant attributes extracted from the CSV file such as shipment details, timestamps, and indicators of potential fraud.\n",
        "\n",
        "\n",
        "Step 2 - Import Shipment Data into the Database: Use a database interaction library (such as psycopg2 for Python) or a database management tool (like pgAdmin) to connect to your Postgres database and import the cleaned and transformed shipment data from the CSV file into the corresponding tables. \n",
        "\n",
        "Step 3 - Create API Layer: We can now create an API layer that interacts with the database and provides endpoints for managing alerts using Flask. This layer will handle requests from external users to create, edit, and delete alert criteria. \n",
        "\n",
        "Step 4 - Design UI for user alert creation: There are various ways via which users can create alerts for potential fraudulent shipments. For this project, a simple user interface where they can interact with the system and provide inputs is ideal for non-tech savvy users. This can be a web-based UI built with HTML, CSS, and JavaScript or a desktop application. The UI would include input fields or forms where users can enter the alert criteria. Upon submitting the form, the UI would send a POST request to the alerts endpoint with the input values.\n",
        "\n",
        " \n",
        "Step 5 - Define Endpoints For Each Alert: On receiving the POST request, an endpoint will be created for each alert criterion by defining the logic. For example, if a user creates an alert for when a shipment contains more than 10000 tonnes, an endpoint to trigger an alert whenever this condition is true is created. The code below shows how this criteria is used to create an alert which notifies the user by SMS when more than 10000 tonnes is shipped."
      ],
      "metadata": {
        "id": "dhqC22oMuu1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwuZxKuQwG7M",
        "outputId": "ba50e9d0-7623-4c30-e1cc-acaea9f9ff23"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.4)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.3.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flask_sqlalchemy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcqPpce-wTY2",
        "outputId": "dd8b33d2-cd59-496a-b4b9-c8be0e220d93"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask_sqlalchemy in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: Flask>=2.2 in /usr/local/lib/python3.10/dist-packages (from flask_sqlalchemy) (2.2.4)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.18 in /usr/local/lib/python3.10/dist-packages (from flask_sqlalchemy) (2.0.10)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=2.2->flask_sqlalchemy) (2.3.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=2.2->flask_sqlalchemy) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=2.2->flask_sqlalchemy) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=2.2->flask_sqlalchemy) (8.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.18->flask_sqlalchemy) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.18->flask_sqlalchemy) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=2.2->flask_sqlalchemy) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HbcXLRQQtJA1"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, jsonify, request\n",
        "from flask_sqlalchemy import SQLAlchemy\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Set the database connection URI\n",
        "app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://username:password@localhost/database'\n",
        "\n",
        "db = SQLAlchemy(app)\n",
        "\n",
        "# Define the Shipment model\n",
        "class Shipment(db.Model):\n",
        "    id = db.Column(db.Integer, primary_key=True)\n",
        "    tonnes = db.Column(db.Float)\n",
        "    # Other columns and relationships\n",
        "\n",
        "@app.route('/shipments', methods=['POST'])\n",
        "def create_shipment():\n",
        "    # Get the shipment data from the request\n",
        "    data = request.get_json()\n",
        "    tonnes = data['tonnes']\n",
        "\n",
        "    # Create a new shipment\n",
        "    shipment = Shipment(tonnes=tonnes)\n",
        "    db.session.add(shipment)\n",
        "    db.session.commit()\n",
        "\n",
        "    # Check if the number of tonnes is above 10000\n",
        "    if tonnes > 10000:\n",
        "        # Alert the user\n",
        "        alert_message = f\"Shipment with {tonnes} tonnes is above 10000 tonnes!\"\n",
        "        send_alert(alert_message)\n",
        "\n",
        "    # Return a success response\n",
        "    return jsonify({'message': 'Shipment created successfully'})\n",
        "\n",
        "def send_alert(message):\n",
        "    # Dummy implementation for sending an SMS notification\n",
        "    # Replace with your actual SMS notification logic or service integration\n",
        "    dummy_phone_number = \"+1234567890\"\n",
        "    sms_message = f\"ALERT: {message}\"\n",
        "    send_sms(dummy_phone_number, sms_message)\n",
        "\n",
        "def send_sms(phone_number, message):\n",
        "    # Logic to send an SMS to the specified phone number with the given message\n",
        "    # Implement your actual SMS notification logic or service integration here\n",
        "    # Example:\n",
        "    print(f\"Sending SMS to {phone_number}: {message}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Deleting Alerts: Users can similarly delete alerts on the UI. In the code below, the endpoint /alerts/<alert_id> is used to handle the DELETE request for deleting an alert. The <alert_id> is a route parameter that represents the unique identifier of the alert to be deleted. Upon receiving the DELETE request, the endpoint retrieves the corresponding alert from the database based on the provided alert_id. If the alert exists, it is deleted from the database. If the alert is not found, a not found response with a 404 status code is returned.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LxtpnJ8QtfCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#syntax for deleting alert \n",
        "@app.route('/alerts/<alert_id>', methods=['DELETE'])\n",
        "def delete_alert(alert_id):\n",
        "    # Retrieve the alert from the database based on the provided alert_id\n",
        "    alert = Alert.query.get(alert_id)\n",
        "\n",
        "    if alert:\n",
        "        # Delete the alert from the database\n",
        "        db.session.delete(alert)\n",
        "        db.session.commit()\n",
        "\n",
        "        # Return a success response\n",
        "        return jsonify({'message': 'Alert deleted successfully'})\n",
        "    else:\n",
        "        # Return a not found response if the alert doesn't exist\n",
        "        return jsonify({'message': 'Alert not found'}), 404"
      ],
      "metadata": {
        "id": "L-v1Ee1Ztedl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Updating alert criteria: Users can also update an existing alert via the UI. In the code below, the endpoint /alerts/<alert_id> is used to handle the PUT or PATCH request for updating an alert. The <alert_id> is a route parameter that represents the unique identifier of the alert to be updated.Upon receiving the request, the endpoint retrieves the corresponding alert from the database based on the provided alert_id. If the alert exists, it checks for updated criteria in the request data. If new criteria are provided, it updates the alert's criteria and saves the changes to the database.If the alert is not found, a not found response with a 404 status code is returned.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "spbk3dRUtvJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#syntax for updating/editing alert \n",
        "@app.route('/alerts/<alert_id>', methods=['PUT', 'PATCH'])\n",
        "def update_alert(alert_id):\n",
        "    # Retrieve the alert from the database based on the provided alert_id\n",
        "    alert = Alert.query.get(alert_id)\n",
        "\n",
        "    if alert:\n",
        "        # Get the updated criteria from the request data\n",
        "        updated_criteria = request.get_json().get('criteria')\n",
        "\n",
        "        # Update the alert criteria if provided\n",
        "        if updated_criteria:\n",
        "            alert.criteria = updated_criteria\n",
        "            db.session.commit()\n",
        "\n",
        "        # Return a success response\n",
        "        return jsonify({'message': 'Alert updated successfully'})\n",
        "    else:\n",
        "        # Return a not found response if the alert doesn't exist\n",
        "        return jsonify({'message': 'Alert not found'}), 404\n",
        "\n"
      ],
      "metadata": {
        "id": "eGDhmV9Bt2gM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Step 6 - Implement Endpoint Logic: Implement the logic for each endpoint. This includes retrieving data from the database, performing necessary operations, and returning appropriate responses. "
      ],
      "metadata": {
        "id": "sQ14pHH2uDK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7 - Establish database Interaction: Integrate the database interaction within the endpoint logic. Use an ORM library like SQLAlchemy to query and manipulate the database."
      ],
      "metadata": {
        "id": "MepjO851uJi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example model for Alert\n",
        "from flask_sqlalchemy import SQLAlchemy\n",
        "\n",
        "\n",
        "class Alert(db.Model):\n",
        "    id = db.Column(db.Integer, primary_key=True)\n",
        "    criteria = db.Column(db.String(255))\n",
        "    # Other columns and relationships\n",
        "\n",
        "# Use the Alert model in endpoint logic"
      ],
      "metadata": {
        "id": "-D_FOAjzuPNa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Step 8 - Run the Application: Run the Flask application by adding the following lines at the end of the Python file."
      ],
      "metadata": {
        "id": "qc5XhNsmuZRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YS2NU-tudtT",
        "outputId": "f10a08e5-31f3-4ba7-d70a-394823899e88"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9 - Test the Endpoints: Start the Flask development server by running python api.py in your command-line interface. Test the defined endpoints using API testing tools like cURL, Postman, or by making HTTP requests from other applications."
      ],
      "metadata": {
        "id": "u46QJt2wug_j"
      }
    }
  ]
}